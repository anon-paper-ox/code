{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bBNKStedWVrK"
      },
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Active Learning with MC Dropout (MNIST) â€” Experiments 5.1 / 5.2\n",
        "# ============================================================\n",
        "\n",
        "import os\n",
        "import random\n",
        "import shutil\n",
        "from dataclasses import dataclass\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, Sampler\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# ðŸ”‡ Silence PyTorch DataLoader cleanup spam (keeps workers=4 + persistent=True)\n",
        "# -------------------------\n",
        "def silence_pytorch_dataloader_cleanup_error():\n",
        "    import torch.utils.data.dataloader as dl\n",
        "    cls = getattr(dl, \"_MultiProcessingDataLoaderIter\", None)\n",
        "    if cls is None:\n",
        "        return\n",
        "    orig_del = cls.__del__\n",
        "    def safe_del(self):\n",
        "        try:\n",
        "            orig_del(self)\n",
        "        except AssertionError:\n",
        "            # \"can only test a child process\"\n",
        "            pass\n",
        "    cls.__del__ = safe_del\n",
        "\n",
        "\n",
        "def silence_multiprocessing_connection_cleanup_error():\n",
        "    import multiprocessing.connection as mp_conn\n",
        "    base = getattr(mp_conn, \"_ConnectionBase\", None)\n",
        "    if base is None:\n",
        "        return\n",
        "    orig_del = base.__del__\n",
        "    def safe_del(self):\n",
        "        try:\n",
        "            orig_del(self)\n",
        "        except OSError as e:\n",
        "            if getattr(e, \"errno\", None) == 9:  # Bad file descriptor\n",
        "                return\n",
        "            raise\n",
        "    base.__del__ = safe_del\n",
        "\n",
        "\n",
        "silence_pytorch_dataloader_cleanup_error()\n",
        "silence_multiprocessing_connection_cleanup_error()\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Experiment constants\n",
        "# -------------------------\n",
        "NUM_CLASSES = 10\n",
        "\n",
        "INITIAL_POINTS_PER_CLASS = 2\n",
        "VAL_SIZE = 100\n",
        "\n",
        "NUM_REPEATS = 3\n",
        "\n",
        "TOTAL_ROUNDS = 100\n",
        "ACQUISITION_SIZE = 10\n",
        "\n",
        "MC_SAMPLES = 20\n",
        "\n",
        "TRAIN_STEPS = 1000\n",
        "TUNE_STEPS  = 500\n",
        "\n",
        "LR = 1e-3\n",
        "WEIGHT_DECAY_GRID = [0.0, 1e-7, 1e-6, 1e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2]\n",
        "\n",
        "EPS = 1e-10\n",
        "SEED = 42\n",
        "\n",
        "BATCH_SIZE_TRAIN = 64\n",
        "BATCH_SIZE_EVAL  = 512\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "PIN_MEMORY = (DEVICE.type == \"cuda\")\n",
        "\n",
        "TRANSFORM = transforms.ToTensor()\n",
        "MNIST = datasets.MNIST\n",
        "\n",
        "# âœ… requested\n",
        "NUM_WORKERS = 4\n",
        "PERSISTENT_WORKERS = True\n",
        "PREFETCH_FACTOR = 4\n",
        "\n",
        "LOG_EVERY_ROUNDS = 10\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Experiment routing\n",
        "# -------------------------\n",
        "EXP_5_1_STRATEGIES = [\"BALD\", \"VarRatios\", \"MaxEntropy\", \"MeanSTD\", \"Random\"]\n",
        "EXP_5_2_STRATEGIES = [\"BALD\", \"VarRatios\", \"MaxEntropy\"]\n",
        "MODES_5_2 = [\"bayes\", \"det\"]\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Quick smoke test config\n",
        "# -------------------------\n",
        "QUICK_TEST = False\n",
        "\n",
        "if QUICK_TEST:\n",
        "    NUM_REPEATS = 2\n",
        "    TOTAL_ROUNDS = 20\n",
        "    ACQUISITION_SIZE = 10\n",
        "    TRAIN_STEPS = 200\n",
        "    TUNE_STEPS  = 100\n",
        "    MC_SAMPLES  = 10\n",
        "    WEIGHT_DECAY_GRID = [0.0, 1e-4, 1e-2]\n",
        "    LOG_EVERY_ROUNDS = 1\n",
        "\n",
        "    EXP_5_1_STRATEGIES = []\n",
        "    EXP_5_2_STRATEGIES = [\"BALD\"]\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Paths / IO\n",
        "# -------------------------\n",
        "def get_results_dir(experiment_id: str) -> str:\n",
        "    return f\"results_{experiment_id}\"\n",
        "\n",
        "def is_bayesian_mode(mode: str) -> bool:\n",
        "    return mode == \"bayes\"\n",
        "\n",
        "def results_path(experiment_id: str, strategy: str, mode: str | None = None) -> str:\n",
        "    base = get_results_dir(experiment_id)\n",
        "    os.makedirs(base, exist_ok=True)\n",
        "    if mode is None:\n",
        "        return os.path.join(base, f\"results_{strategy}.npz\")\n",
        "    return os.path.join(base, f\"results_{strategy}_{mode}.npz\")\n",
        "\n",
        "def summary_path(experiment_id: str) -> str:\n",
        "    base = get_results_dir(experiment_id)\n",
        "    os.makedirs(base, exist_ok=True)\n",
        "    return os.path.join(base, f\"summary_{experiment_id}.npz\")\n",
        "\n",
        "def figure_path_5_1(experiment_id: str) -> str:\n",
        "    base = get_results_dir(experiment_id)\n",
        "    os.makedirs(base, exist_ok=True)\n",
        "    return os.path.join(base, f\"figure_{experiment_id}.png\")\n",
        "\n",
        "def figure_path_5_2_strategy(experiment_id: str, strategy: str) -> str:\n",
        "    base = get_results_dir(experiment_id)\n",
        "    os.makedirs(base, exist_ok=True)\n",
        "    return os.path.join(base, f\"figure_5_2_{strategy}.png\")\n",
        "\n",
        "def _load_npz_or_raise(path: str):\n",
        "    if not os.path.exists(path):\n",
        "        raise FileNotFoundError(f\"Missing results file: {path}\")\n",
        "    return np.load(path, allow_pickle=True)\n",
        "\n",
        "def atomic_savez(path, **kwargs):\n",
        "    tmp = path + \".tmp.npz\"\n",
        "    np.savez(tmp, **kwargs)\n",
        "    os.replace(tmp, path)\n",
        "\n",
        "def reuse_5_1_bayes_as_5_2_bayes(src_exp=\"5_1\", dst_exp=\"5_2\", strategies=None, overwrite=False):\n",
        "    if strategies is None:\n",
        "        strategies = [\"BALD\", \"VarRatios\", \"MaxEntropy\"]\n",
        "\n",
        "    for strat in strategies:\n",
        "        src = results_path(src_exp, strat, mode=None)\n",
        "        dst = results_path(dst_exp, strat, mode=\"bayes\")\n",
        "\n",
        "        if not os.path.exists(src):\n",
        "            print(f\"[REUSE-SKIP] Missing source (run 5.1 first): {src}\")\n",
        "            continue\n",
        "\n",
        "        if os.path.exists(dst) and not overwrite:\n",
        "            print(f\"[REUSE-KEEP] Already exists: {dst}\")\n",
        "            continue\n",
        "\n",
        "        os.makedirs(os.path.dirname(dst), exist_ok=True)\n",
        "        shutil.copy2(src, dst)\n",
        "        print(f\"[REUSE-COPY] {src} -> {dst}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Seeding helpers\n",
        "# -------------------------\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "def set_torch_seed_only(seed: int):\n",
        "    torch.manual_seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Samplers\n",
        "# -------------------------\n",
        "class MutableSubsetSequentialSampler(Sampler[int]):\n",
        "    def __init__(self, indices=None):\n",
        "        self.indices = list(indices) if indices is not None else []\n",
        "\n",
        "    def set_indices(self, indices):\n",
        "        self.indices = list(indices)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return iter(self.indices)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.indices)\n",
        "\n",
        "\n",
        "class MutableSubsetReplacementSampler(Sampler[int]):\n",
        "    \"\"\"\n",
        "    Samples WITH replacement from indices, returning exactly `num_samples`.\n",
        "    With drop_last=True, this yields exactly num_samples / batch_size batches.\n",
        "    \"\"\"\n",
        "    def __init__(self, indices=None, seed: int = 0, num_samples: int = 0):\n",
        "        self.indices = list(indices) if indices is not None else []\n",
        "        self.seed = int(seed)\n",
        "        self.num_samples = int(num_samples)\n",
        "\n",
        "    def set_indices(self, indices):\n",
        "        self.indices = list(indices)\n",
        "\n",
        "    def set_seed(self, seed: int):\n",
        "        self.seed = int(seed)\n",
        "\n",
        "    def set_num_samples(self, num_samples: int):\n",
        "        self.num_samples = int(num_samples)\n",
        "\n",
        "    def __iter__(self):\n",
        "        if len(self.indices) == 0 or self.num_samples <= 0:\n",
        "            return iter([])\n",
        "\n",
        "        g = torch.Generator()\n",
        "        g.manual_seed(self.seed)\n",
        "\n",
        "        draws = torch.randint(0, len(self.indices), (self.num_samples,), generator=g).tolist()\n",
        "        return (self.indices[i] for i in draws)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SharedLoaders:\n",
        "    train_loader: DataLoader\n",
        "    train_sampler: MutableSubsetReplacementSampler\n",
        "    val_loader: DataLoader\n",
        "    val_sampler: MutableSubsetSequentialSampler\n",
        "    pool_loader: DataLoader\n",
        "    pool_sampler: MutableSubsetSequentialSampler\n",
        "\n",
        "\n",
        "def _dataloader_kwargs():\n",
        "    kwargs = dict(\n",
        "        pin_memory=PIN_MEMORY,\n",
        "        num_workers=NUM_WORKERS,\n",
        "        persistent_workers=PERSISTENT_WORKERS,\n",
        "    )\n",
        "    if NUM_WORKERS > 0:\n",
        "        kwargs[\"prefetch_factor\"] = PREFETCH_FACTOR\n",
        "    return kwargs\n",
        "\n",
        "\n",
        "def build_shared_loaders(train_full) -> SharedLoaders:\n",
        "    train_sampler = MutableSubsetReplacementSampler(indices=[], seed=0, num_samples=0)\n",
        "    val_sampler   = MutableSubsetSequentialSampler(indices=[])\n",
        "    pool_sampler  = MutableSubsetSequentialSampler(indices=[])\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_full,\n",
        "        batch_size=BATCH_SIZE_TRAIN,\n",
        "        shuffle=False,\n",
        "        sampler=train_sampler,\n",
        "        drop_last=True,\n",
        "        **_dataloader_kwargs(),\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        train_full,\n",
        "        batch_size=BATCH_SIZE_EVAL,\n",
        "        shuffle=False,\n",
        "        sampler=val_sampler,\n",
        "        **_dataloader_kwargs(),\n",
        "    )\n",
        "    pool_loader = DataLoader(\n",
        "        train_full,\n",
        "        batch_size=BATCH_SIZE_EVAL,\n",
        "        shuffle=False,\n",
        "        sampler=pool_sampler,\n",
        "        **_dataloader_kwargs(),\n",
        "    )\n",
        "\n",
        "    return SharedLoaders(\n",
        "        train_loader=train_loader, train_sampler=train_sampler,\n",
        "        val_loader=val_loader,     val_sampler=val_sampler,\n",
        "        pool_loader=pool_loader,   pool_sampler=pool_sampler,\n",
        "    )\n",
        "\n",
        "\n",
        "def make_test_loader(test_dataset):\n",
        "    return DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE_EVAL,\n",
        "        shuffle=False,\n",
        "        **_dataloader_kwargs(),\n",
        "    )\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Data split\n",
        "# -------------------------\n",
        "def split_indices(train_full, seed):\n",
        "    set_seed(seed)\n",
        "    targets = train_full.targets.cpu().numpy()\n",
        "\n",
        "    by_class = [[] for _ in range(NUM_CLASSES)]\n",
        "    for i, y in enumerate(targets):\n",
        "        by_class[int(y)].append(i)\n",
        "\n",
        "    init_idx = []\n",
        "    for c in range(NUM_CLASSES):\n",
        "        chosen = random.sample(by_class[c], INITIAL_POINTS_PER_CLASS)\n",
        "        init_idx.extend(chosen)\n",
        "        for idx in chosen:\n",
        "            by_class[c].remove(idx)\n",
        "\n",
        "    remaining = []\n",
        "    for c in range(NUM_CLASSES):\n",
        "        remaining.extend(by_class[c])\n",
        "\n",
        "    random.shuffle(remaining)\n",
        "    val_idx  = remaining[:VAL_SIZE]\n",
        "    pool_idx = remaining[VAL_SIZE:]\n",
        "    return init_idx, val_idx, pool_idx\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Model\n",
        "# -------------------------\n",
        "class SimpleBCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=4)\n",
        "        self.conv2 = nn.Conv2d(32, 32, kernel_size=4)\n",
        "        self.pool  = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.fc1 = nn.Linear(32 * 11 * 11, 128)\n",
        "        self.fc2 = nn.Linear(128, NUM_CLASSES)\n",
        "\n",
        "        self.drop1 = nn.Dropout(p=0.25)\n",
        "        self.drop2 = nn.Dropout(p=0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.conv1(x))\n",
        "        x = F.relu(self.conv2(x))\n",
        "        x = self.pool(x)\n",
        "        x = self.drop1(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.drop2(x)\n",
        "        return self.fc2(x)\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Training\n",
        "# -------------------------\n",
        "def train_model(train_idx, weight_decay, steps, shared: SharedLoaders,\n",
        "               init_state_dict=None, init_seed=None, shuffle_seed=None):\n",
        "\n",
        "    if init_seed is not None:\n",
        "        set_torch_seed_only(init_seed)\n",
        "\n",
        "    model = SimpleBCNN()\n",
        "    if init_state_dict is not None:\n",
        "        model.load_state_dict(init_state_dict)\n",
        "    model = model.to(DEVICE)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LR, weight_decay=weight_decay)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    shared.train_sampler.set_indices(train_idx)\n",
        "    shared.train_sampler.set_seed(shuffle_seed if shuffle_seed is not None else 0)\n",
        "    shared.train_sampler.set_num_samples(steps * BATCH_SIZE_TRAIN)\n",
        "\n",
        "    model.train()\n",
        "    for x, y in shared.train_loader:\n",
        "        x = x.to(DEVICE, non_blocking=True)\n",
        "        y = y.to(DEVICE, non_blocking=True)\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        logits = model(x)\n",
        "        loss = loss_fn(logits, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Predictive helpers\n",
        "# -------------------------\n",
        "def predictive_probs(model, x, T=MC_SAMPLES, bayesian=True):\n",
        "    if bayesian:\n",
        "        model.train()\n",
        "        B = x.size(0)\n",
        "        x_rep = x.repeat(T, 1, 1, 1)\n",
        "        logits = model(x_rep).view(T, B, NUM_CLASSES)\n",
        "        probs_T = F.softmax(logits, dim=2)\n",
        "        return probs_T.mean(dim=0), probs_T\n",
        "    else:\n",
        "        model.eval()\n",
        "        logits = model(x)\n",
        "        return F.softmax(logits, dim=1), None\n",
        "\n",
        "\n",
        "def accuracy(model, loader, T=MC_SAMPLES, bayesian=True):\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "            mean_probs, _ = predictive_probs(model, x, T=T, bayesian=bayesian)\n",
        "            preds = mean_probs.argmax(dim=1)\n",
        "            correct += (preds == y).sum().item()\n",
        "            total += x.size(0)\n",
        "    return correct / total\n",
        "\n",
        "\n",
        "def mc_nll(model, loader, T=MC_SAMPLES):\n",
        "    model.train()\n",
        "    total_nll = 0.0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "            B = x.size(0)\n",
        "\n",
        "            x_rep = x.repeat(T, 1, 1, 1)\n",
        "            logits = model(x_rep).view(T, B, NUM_CLASSES)\n",
        "            probs = F.softmax(logits, dim=2).mean(dim=0).clamp_min(EPS)\n",
        "\n",
        "            total_nll += F.nll_loss(probs.log(), y, reduction=\"sum\").item()\n",
        "            total += B\n",
        "    return total_nll / total\n",
        "\n",
        "\n",
        "def det_nll(model, loader):\n",
        "    model.eval()\n",
        "    total_nll = 0.0\n",
        "    total = 0\n",
        "    with torch.inference_mode():\n",
        "        for x, y in loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            y = y.to(DEVICE, non_blocking=True)\n",
        "            logits = model(x)\n",
        "            log_probs = F.log_softmax(logits, dim=1)\n",
        "            total_nll += F.nll_loss(log_probs, y, reduction=\"sum\").item()\n",
        "            total += x.size(0)\n",
        "    return total_nll / total\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Acquisition scoring\n",
        "# -------------------------\n",
        "def score_pool(model, pool_idx, strategy, shared: SharedLoaders, T=MC_SAMPLES, bayesian=True):\n",
        "    shared.pool_sampler.set_indices(pool_idx)\n",
        "\n",
        "    all_scores = []\n",
        "    with torch.inference_mode():\n",
        "        for x, _ in shared.pool_loader:\n",
        "            x = x.to(DEVICE, non_blocking=True)\n",
        "            mean_probs, probs_T = predictive_probs(model, x, T=T, bayesian=bayesian)\n",
        "\n",
        "            if strategy == \"MaxEntropy\":\n",
        "                s = -(mean_probs * torch.log(mean_probs + EPS)).sum(dim=1)\n",
        "            elif strategy == \"VarRatios\":\n",
        "                s = 1.0 - mean_probs.max(dim=1).values\n",
        "            elif strategy == \"MeanSTD\":\n",
        "                if not bayesian:\n",
        "                    s = -(mean_probs * torch.log(mean_probs + EPS)).sum(dim=1)\n",
        "                else:\n",
        "                    s = probs_T.std(dim=0, unbiased=False).mean(dim=1)\n",
        "            elif strategy == \"BALD\":\n",
        "                if not bayesian:\n",
        "                    s = torch.zeros(mean_probs.size(0), device=mean_probs.device)\n",
        "                else:\n",
        "                    ent_mean = -(mean_probs * torch.log(mean_probs + EPS)).sum(dim=1)\n",
        "                    ent_each = -(probs_T * torch.log(probs_T + EPS)).sum(dim=2).mean(dim=0)\n",
        "                    s = ent_mean - ent_each\n",
        "            else:\n",
        "                raise ValueError(strategy)\n",
        "\n",
        "            all_scores.append(s.cpu())\n",
        "\n",
        "    return torch.cat(all_scores, dim=0).numpy()\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Active learning loop\n",
        "# -------------------------\n",
        "def run_active_learning_once(test_loader, shared: SharedLoaders,\n",
        "                            init_idx, pool_idx, weight_decay, strategy, seed, bayesian=True):\n",
        "\n",
        "    set_seed(seed)\n",
        "    train_idx = list(init_idx)\n",
        "    pool_idx  = list(pool_idx)\n",
        "\n",
        "    acc_curve = []\n",
        "\n",
        "    train_seed0 = 10_000 * seed + 0\n",
        "    model = train_model(train_idx, weight_decay, TRAIN_STEPS, shared,\n",
        "                        init_seed=train_seed0, shuffle_seed=train_seed0)\n",
        "    acc_curve.append(accuracy(model, test_loader, T=MC_SAMPLES, bayesian=bayesian))\n",
        "\n",
        "    for _round in range(1, TOTAL_ROUNDS + 1):\n",
        "        if strategy == \"Random\":\n",
        "            acquired = random.sample(pool_idx, ACQUISITION_SIZE)\n",
        "        else:\n",
        "            scores = score_pool(model, pool_idx, strategy, shared, T=MC_SAMPLES, bayesian=bayesian)\n",
        "            k = ACQUISITION_SIZE\n",
        "            top_local = np.argpartition(scores, -k)[-k:]\n",
        "            top_local = top_local[np.argsort(scores[top_local])[::-1]]\n",
        "            acquired = [pool_idx[i] for i in top_local]\n",
        "\n",
        "        acquired_set = set(acquired)\n",
        "        train_idx.extend(acquired)\n",
        "        pool_idx = [i for i in pool_idx if i not in acquired_set]\n",
        "\n",
        "        train_seed = 10_000 * seed + _round\n",
        "        model = train_model(train_idx, weight_decay, TRAIN_STEPS, shared,\n",
        "                            init_seed=train_seed, shuffle_seed=train_seed)\n",
        "        acc_curve.append(accuracy(model, test_loader, T=MC_SAMPLES, bayesian=bayesian))\n",
        "\n",
        "        if _round % LOG_EVERY_ROUNDS == 0 or _round == TOTAL_ROUNDS:\n",
        "            print(f\"[{strategy} | bayes={bayesian} | seed={seed}] round {_round}/{TOTAL_ROUNDS} acc={acc_curve[-1]*100:.2f}%\")\n",
        "\n",
        "    return acc_curve\n",
        "\n",
        "\n",
        "def tune_weight_decay(init_idx, val_idx, seed, shared: SharedLoaders, bayesian=True):\n",
        "    shared.val_sampler.set_indices(val_idx)\n",
        "\n",
        "    set_seed(seed)\n",
        "    base_model = SimpleBCNN()\n",
        "    init_state = {k: v.clone() for k, v in base_model.state_dict().items()}\n",
        "\n",
        "    best_wd = None\n",
        "    best_val = float(\"inf\")\n",
        "    tol = 1e-6\n",
        "\n",
        "    for wd in WEIGHT_DECAY_GRID:\n",
        "        model = train_model(init_idx, wd, TUNE_STEPS, shared,\n",
        "                            init_state_dict=init_state,\n",
        "                            init_seed=12345 + seed,\n",
        "                            shuffle_seed=67890 + seed)\n",
        "\n",
        "        set_torch_seed_only(99999 + seed)\n",
        "        val = mc_nll(model, shared.val_loader, T=MC_SAMPLES) if bayesian else det_nll(model, shared.val_loader)\n",
        "        print(f\"WD={wd:.0e} | val={'MC-NLL' if bayesian else 'NLL'}={val:.4f}\")\n",
        "\n",
        "        if (val < best_val - tol) or (abs(val - best_val) <= tol and (best_wd is None or wd < best_wd)):\n",
        "            best_val = val\n",
        "            best_wd = wd\n",
        "\n",
        "    print(f\"Selected best weight decay: {best_wd:.0e} (val={best_val:.4f})\")\n",
        "    return best_wd\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Run one strategy\n",
        "# -------------------------\n",
        "def run_one_strategy(experiment_id, strategy, train_full, test_loader, shared: SharedLoaders, overwrite=False, mode=None):\n",
        "    bayesian = True if mode is None else is_bayesian_mode(mode)\n",
        "    save_path = results_path(experiment_id, strategy, mode)\n",
        "\n",
        "    if overwrite and os.path.exists(save_path):\n",
        "        os.remove(save_path)\n",
        "        print(f\"[OVERWRITE] Deleted existing {save_path}\")\n",
        "\n",
        "    curves_done, wds_done, seeds_done = [], [], []\n",
        "\n",
        "    if (not overwrite) and os.path.exists(save_path):\n",
        "        old = np.load(save_path, allow_pickle=True)\n",
        "        curves_done = list(old[\"curves\"])\n",
        "        wds_done    = old[\"best_wd_per_repeat\"].tolist()\n",
        "        seeds_done  = old[\"seeds\"].tolist()\n",
        "        print(f\"[RESUME] Found {len(curves_done)}/{NUM_REPEATS} repeats in {save_path}\")\n",
        "\n",
        "    start_repeat = len(curves_done)\n",
        "\n",
        "    for r in range(start_repeat, NUM_REPEATS):\n",
        "        repeat_seed = SEED + r\n",
        "        print(\"\\n\" + \"=\" * 60)\n",
        "        print(f\"Experiment={experiment_id} | Strategy={strategy} | Mode={mode or 'bayes'} | Repeat {r+1}/{NUM_REPEATS} | seed={repeat_seed}\")\n",
        "        print(\"=\" * 60)\n",
        "\n",
        "        init_idx, val_idx, pool_idx = split_indices(train_full, repeat_seed)\n",
        "        best_wd = tune_weight_decay(init_idx, val_idx, seed=repeat_seed, shared=shared, bayesian=bayesian)\n",
        "\n",
        "        acc_curve = run_active_learning_once(\n",
        "            test_loader=test_loader,\n",
        "            shared=shared,\n",
        "            init_idx=init_idx,\n",
        "            pool_idx=pool_idx,\n",
        "            weight_decay=best_wd,\n",
        "            strategy=strategy,\n",
        "            seed=repeat_seed,\n",
        "            bayesian=bayesian,\n",
        "        )\n",
        "\n",
        "        curves_done.append(np.array(acc_curve, dtype=np.float32))\n",
        "        wds_done.append(best_wd)\n",
        "        seeds_done.append(repeat_seed)\n",
        "\n",
        "        atomic_savez(\n",
        "            save_path,\n",
        "            strategy=strategy,\n",
        "            mode=(mode if mode is not None else \"bayes\"),\n",
        "            curves=np.stack(curves_done, axis=0).astype(np.float32),\n",
        "            best_wd_per_repeat=np.array(wds_done, dtype=np.float64),\n",
        "            seeds=np.array(seeds_done, dtype=np.int64),\n",
        "        )\n",
        "        print(f\"[SAVED] {save_path} (repeats completed: {len(curves_done)}/{NUM_REPEATS})\")\n",
        "\n",
        "    print(f\"\\n[DONE] Experiment={experiment_id} Strategy={strategy} Mode={mode or 'bayes'} complete.\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Summaries + plots\n",
        "# -------------------------\n",
        "def acquired_axis():\n",
        "    return np.arange(0, (TOTAL_ROUNDS + 1) * ACQUISITION_SIZE, ACQUISITION_SIZE)\n",
        "\n",
        "def summarize_5_1(experiment_id: str):\n",
        "    x = acquired_axis()\n",
        "    results = {}\n",
        "    for strat in EXP_5_1_STRATEGIES:\n",
        "        data = _load_npz_or_raise(results_path(experiment_id, strat, mode=None))\n",
        "        curves = data[\"curves\"].astype(np.float32)\n",
        "        results[strat] = {\"curves\": curves, \"mean\": curves.mean(axis=0), \"std\": curves.std(axis=0)}\n",
        "    atomic_savez(summary_path(experiment_id), x_acquired=x, results=results)\n",
        "\n",
        "def table_5_1_thresholds(experiment_id: str, error_targets=(0.10, 0.05)):\n",
        "    data = _load_npz_or_raise(summary_path(experiment_id))\n",
        "    x = data[\"x_acquired\"]\n",
        "    results = data[\"results\"].item()\n",
        "\n",
        "    out = {}\n",
        "    for strat, stats in results.items():\n",
        "        mean_err = 1.0 - stats[\"mean\"]\n",
        "        out[strat] = {}\n",
        "        for e in error_targets:\n",
        "            hit = np.where(mean_err <= e)[0]\n",
        "            out[strat][e] = int(x[hit[0]]) if len(hit) > 0 else None\n",
        "\n",
        "    print(\"\\nTable 1 style (acquired images to reach error):\")\n",
        "    header = \"strategy\".ljust(12) + \"10% err\".rjust(10) + \"5% err\".rjust(10)\n",
        "    print(header)\n",
        "    print(\"-\" * len(header))\n",
        "    for strat in EXP_5_1_STRATEGIES:\n",
        "        if strat not in out:\n",
        "            continue\n",
        "        print(strat.ljust(12) + str(out[strat][0.10]).rjust(10) + str(out[strat][0.05]).rjust(10))\n",
        "\n",
        "    atomic_savez(os.path.join(get_results_dir(experiment_id), \"table1_thresholds.npz\"),\n",
        "                 table=out, x_acquired=x)\n",
        "    return out\n",
        "\n",
        "def plot_figure_5_1(experiment_id: str):\n",
        "    data = _load_npz_or_raise(summary_path(experiment_id))\n",
        "    x = data[\"x_acquired\"]\n",
        "    results = data[\"results\"].item()\n",
        "\n",
        "    label_map = {\n",
        "        \"BALD\": \"BALD\",\n",
        "        \"VarRatios\": \"Var Ratios\",\n",
        "        \"MaxEntropy\": \"Max Entropy\",\n",
        "        \"MeanSTD\": \"Mean STD\",\n",
        "        \"Random\": \"Random\",\n",
        "    }\n",
        "\n",
        "    plt.figure(figsize=(6.2, 4.8))\n",
        "    ax = plt.gca()\n",
        "    ax.set_axisbelow(True)\n",
        "    ax.grid(True, which=\"major\", linestyle=\"--\", linewidth=1.0, alpha=0.25)\n",
        "\n",
        "    for strat in EXP_5_1_STRATEGIES:\n",
        "        mean = 100.0 * results[strat][\"mean\"]\n",
        "        std  = 100.0 * results[strat][\"std\"]\n",
        "        ax.plot(x, mean, linewidth=2.2, label=label_map.get(strat, strat))\n",
        "        ax.fill_between(x, mean - std, mean + std, alpha=0.15, linewidth=0)\n",
        "\n",
        "    ax.set_xlabel(\"Acquired images\")\n",
        "    ax.set_ylabel(\"Test accuracy (%)\")\n",
        "    ax.set_xlim(0, int(x[-1]))\n",
        "    ax.set_ylim(80, 100)\n",
        "    ax.set_yticks(np.arange(80, 101, 2))\n",
        "    ax.legend(loc=\"lower right\", frameon=True, fancybox=False, framealpha=1.0, fontsize=9)\n",
        "\n",
        "    out = figure_path_5_1(experiment_id)\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
        "    plt.close()\n",
        "    print(f\"Saved {out}\")\n",
        "\n",
        "def summarize_5_2(experiment_id: str):\n",
        "    x = acquired_axis()\n",
        "    results = {}\n",
        "    for strat in EXP_5_2_STRATEGIES:\n",
        "        results[strat] = {}\n",
        "        for mode in MODES_5_2:\n",
        "            data = _load_npz_or_raise(results_path(experiment_id, strat, mode=mode))\n",
        "            curves = data[\"curves\"].astype(np.float32)\n",
        "            results[strat][mode] = {\"curves\": curves, \"mean\": curves.mean(axis=0), \"std\": curves.std(axis=0)}\n",
        "    atomic_savez(summary_path(experiment_id), x_acquired=x, results=results)\n",
        "\n",
        "def plot_figure_5_2(experiment_id: str):\n",
        "    data = _load_npz_or_raise(summary_path(experiment_id))\n",
        "    x = data[\"x_acquired\"]\n",
        "    results = data[\"results\"].item()\n",
        "\n",
        "    BAYES_COLOR = \"#d62728\"\n",
        "    DET_COLOR   = \"#1f77b4\"\n",
        "    BAND_ALPHA  = 0.18\n",
        "\n",
        "    BAYES_LABEL = \"Bayesian CNN\"\n",
        "    DET_LABEL   = \"Deterministic CNN\"\n",
        "\n",
        "    titles = {\"BALD\": \"(a) BALD\", \"VarRatios\": \"(b) Var Ratios\", \"MaxEntropy\": \"(c) Max Entropy\"}\n",
        "\n",
        "    for strat in EXP_5_2_STRATEGIES:\n",
        "        plt.figure(figsize=(6.2, 4.8))\n",
        "        ax = plt.gca()\n",
        "        ax.set_axisbelow(True)\n",
        "        ax.grid(True, which=\"major\", linestyle=\"--\", linewidth=1.0, alpha=0.25)\n",
        "\n",
        "        bayes_mean = 100.0 * results[strat][\"bayes\"][\"mean\"]\n",
        "        bayes_std  = 100.0 * results[strat][\"bayes\"][\"std\"]\n",
        "        det_mean   = 100.0 * results[strat][\"det\"][\"mean\"]\n",
        "        det_std    = 100.0 * results[strat][\"det\"][\"std\"]\n",
        "\n",
        "        ax.plot(x, bayes_mean, color=BAYES_COLOR, linewidth=2.5, label=BAYES_LABEL)\n",
        "        ax.fill_between(x, bayes_mean - bayes_std, bayes_mean + bayes_std, color=BAYES_COLOR, alpha=BAND_ALPHA, linewidth=0)\n",
        "\n",
        "        ax.plot(x, det_mean, color=DET_COLOR, linewidth=2.5, label=DET_LABEL)\n",
        "        ax.fill_between(x, det_mean - det_std, det_mean + det_std, color=DET_COLOR, alpha=BAND_ALPHA, linewidth=0)\n",
        "\n",
        "        ax.set_title(titles.get(strat, strat), fontsize=12)\n",
        "        ax.set_xlabel(\"Acquired images\")\n",
        "        ax.set_ylabel(\"Test accuracy (%)\")\n",
        "        ax.set_ylim(80, 100)\n",
        "        ax.set_yticks(np.arange(80, 101, 2))\n",
        "\n",
        "        max_x = int(x[-1])\n",
        "        ax.set_xlim(0, max_x)\n",
        "        ax.set_xticks(np.arange(0, max_x + 1, 100))\n",
        "\n",
        "        ax.legend(loc=\"lower right\", frameon=True, fancybox=False, framealpha=1.0, fontsize=9)\n",
        "\n",
        "        out = figure_path_5_2_strategy(experiment_id, strat)\n",
        "        plt.tight_layout()\n",
        "        plt.savefig(out, dpi=200, bbox_inches=\"tight\")\n",
        "        plt.close()\n",
        "        print(f\"Saved {out}\")\n",
        "\n",
        "\n",
        "# -------------------------\n",
        "# Main\n",
        "# -------------------------\n",
        "if __name__ == \"__main__\":\n",
        "    train_full = MNIST(root=\"./data\", train=True, download=True, transform=TRANSFORM)\n",
        "    test_dataset = MNIST(root=\"./data\", train=False, download=True, transform=TRANSFORM)\n",
        "\n",
        "    shared = build_shared_loaders(train_full)\n",
        "    test_loader = make_test_loader(test_dataset)\n",
        "\n",
        "    if QUICK_TEST:\n",
        "        exp_5_2 = \"5_2_smoke\"\n",
        "        for strategy in EXP_5_2_STRATEGIES:\n",
        "            for mode in MODES_5_2:\n",
        "                run_one_strategy(exp_5_2, strategy, train_full, test_loader, shared=shared, overwrite=True, mode=mode)\n",
        "        summarize_5_2(exp_5_2)\n",
        "        plot_figure_5_2(exp_5_2)\n",
        "\n",
        "    else:\n",
        "        # 5.1\n",
        "        for strategy in EXP_5_1_STRATEGIES:\n",
        "            run_one_strategy(\"5_1\", strategy, train_full, test_loader, shared=shared, overwrite=True, mode=None)\n",
        "        summarize_5_1(\"5_1\")\n",
        "        table_5_1_thresholds(\"5_1\", (0.10, 0.05))\n",
        "        plot_figure_5_1(\"5_1\")\n",
        "\n",
        "        # reuse bayes for 5.2\n",
        "        reuse_5_1_bayes_as_5_2_bayes(\"5_1\", \"5_2\", EXP_5_2_STRATEGIES, overwrite=False)\n",
        "\n",
        "        # 5.2 det only\n",
        "        for strategy in EXP_5_2_STRATEGIES:\n",
        "            run_one_strategy(\"5_2\", strategy, train_full, test_loader, shared=shared, overwrite=True, mode=\"det\")\n",
        "        summarize_5_2(\"5_2\")\n",
        "        plot_figure_5_2(\"5_2\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
